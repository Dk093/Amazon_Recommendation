{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2699f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36247b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88ee8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have give a json file which consists of all information about\n",
    "# the products\n",
    "# loading the data using pandas' read_json file.\n",
    "data = pd.read_json('/Users/kaush/Downloads/tops_fashion.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89fdae10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points :  183138 Number of features/variables: 19\n"
     ]
    }
   ],
   "source": [
    "print ('Number of data points : ', data.shape[0],\n",
    "'Number of features/variables:', data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa06b93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sku', 'asin', 'product_type_name', 'formatted_price', 'author',\n",
       "       'color', 'brand', 'publisher', 'availability', 'reviews',\n",
       "       'large_image_url', 'availability_type', 'small_image_url',\n",
       "       'editorial_review', 'title', 'model', 'medium_image_url',\n",
       "       'manufacturer', 'editorial_reivew'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each product/item has 19 features in the raw dataset.\n",
    "data.columns # prints column-names or feature-names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e072b7f1",
   "metadata": {},
   "source": [
    "Of these 19 features, we will be using only 6 features in this workshop. 1. asin ( Amazon\n",
    "standard identification number) 2. brand ( brand to which the product belongs to ) 3. color (\n",
    "Color information of apparel, it can contain many colors as a value ex: red and black stripes ) 4.\n",
    "product_type_name (type of the apperal, ex: SHIRT/TSHIRT ) 5. medium_image_url ( url of the\n",
    "image ) 6. title (title of the product.) 7. formatted_price (price of the product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8dde607",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['asin', 'brand', 'color', 'medium_image_url', 'product_type_name', 'title','formatted_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6929fd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points :  183138 Number of features: 7\n"
     ]
    }
   ],
   "source": [
    "print ('Number of data points : ', data.shape[0], \\\n",
    "'Number of features:', data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cef7088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>brand</th>\n",
       "      <th>color</th>\n",
       "      <th>medium_image_url</th>\n",
       "      <th>product_type_name</th>\n",
       "      <th>title</th>\n",
       "      <th>formatted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B016I2TS4W</td>\n",
       "      <td>FNC7C</td>\n",
       "      <td>None</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>Minions Como Superheroes Ironman Long Sleeve R...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B01N49AI08</td>\n",
       "      <td>FIG Clothing</td>\n",
       "      <td>None</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>FIG Clothing Womens Izo Tunic</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B01JDPCOHO</td>\n",
       "      <td>FIG Clothing</td>\n",
       "      <td>None</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>FIG Clothing Womens Won Top</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B01N19U5H5</td>\n",
       "      <td>Focal18</td>\n",
       "      <td>None</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>Focal18 Sailor Collar Bubble Sleeve Blouse Shi...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B004GSI2OS</td>\n",
       "      <td>FeatherLite</td>\n",
       "      <td>Onyx Black/ Stone</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>Featherlite Ladies' Long Sleeve Stain Resistan...</td>\n",
       "      <td>$26.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin         brand              color  \\\n",
       "0  B016I2TS4W         FNC7C               None   \n",
       "1  B01N49AI08  FIG Clothing               None   \n",
       "2  B01JDPCOHO  FIG Clothing               None   \n",
       "3  B01N19U5H5       Focal18               None   \n",
       "4  B004GSI2OS   FeatherLite  Onyx Black/ Stone   \n",
       "\n",
       "                                    medium_image_url product_type_name  \\\n",
       "0  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "1  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "2  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "3  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "4  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "\n",
       "                                               title formatted_price  \n",
       "0  Minions Como Superheroes Ironman Long Sleeve R...            None  \n",
       "1                      FIG Clothing Womens Izo Tunic            None  \n",
       "2                        FIG Clothing Womens Won Top            None  \n",
       "3  Focal18 Sailor Collar Bubble Sleeve Blouse Shi...            None  \n",
       "4  Featherlite Ladies' Long Sleeve Stain Resistan...          $26.26  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() # prints the top rows in the table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f753d8e",
   "metadata": {},
   "source": [
    "# Missing data for various features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158db813",
   "metadata": {},
   "source": [
    "Basic stats for the feature: product_type_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3faa4f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     183138\n",
      "unique        72\n",
      "top        SHIRT\n",
      "freq      167794\n",
      "Name: product_type_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# We have total 72 unique type of product_type_names\n",
    "print(data['product_type_name'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8904a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SHIRT' 'SWEATER' 'APPAREL' 'OUTDOOR_RECREATION_PRODUCT'\n",
      " 'BOOKS_1973_AND_LATER' 'PANTS' 'HAT' 'SPORTING_GOODS' 'DRESS' 'UNDERWEAR'\n",
      " 'SKIRT' 'OUTERWEAR' 'BRA' 'ACCESSORY' 'ART_SUPPLIES' 'SLEEPWEAR'\n",
      " 'ORCA_SHIRT' 'HANDBAG' 'PET_SUPPLIES' 'SHOES' 'KITCHEN' 'ADULT_COSTUME'\n",
      " 'HOME_BED_AND_BATH' 'MISC_OTHER' 'BLAZER' 'HEALTH_PERSONAL_CARE'\n",
      " 'TOYS_AND_GAMES' 'SWIMWEAR' 'CONSUMER_ELECTRONICS' 'SHORTS' 'HOME'\n",
      " 'AUTO_PART' 'OFFICE_PRODUCTS' 'ETHNIC_WEAR' 'BEAUTY'\n",
      " 'INSTRUMENT_PARTS_AND_ACCESSORIES' 'POWERSPORTS_PROTECTIVE_GEAR' 'SHIRTS'\n",
      " 'ABIS_APPAREL' 'AUTO_ACCESSORY' 'NONAPPARELMISC' 'TOOLS' 'BABY_PRODUCT'\n",
      " 'SOCKSHOSIERY' 'POWERSPORTS_RIDING_SHIRT' 'EYEWEAR' 'SUIT'\n",
      " 'OUTDOOR_LIVING' 'POWERSPORTS_RIDING_JACKET' 'HARDWARE' 'SAFETY_SUPPLY'\n",
      " 'ABIS_DVD' 'VIDEO_DVD' 'GOLF_CLUB' 'MUSIC_POPULAR_VINYL'\n",
      " 'HOME_FURNITURE_AND_DECOR' 'TABLET_COMPUTER' 'GUILD_ACCESSORIES'\n",
      " 'ABIS_SPORTS' 'ART_AND_CRAFT_SUPPLY' 'BAG' 'MECHANICAL_COMPONENTS'\n",
      " 'SOUND_AND_RECORDING_EQUIPMENT' 'COMPUTER_COMPONENT' 'JEWELRY'\n",
      " 'BUILDING_MATERIAL' 'LUGGAGE' 'BABY_COSTUME' 'POWERSPORTS_VEHICLE_PART'\n",
      " 'PROFESSIONAL_HEALTHCARE' 'SEEDS_AND_PLANTS' 'WIRELESS_ACCESSORY']\n"
     ]
    }
   ],
   "source": [
    "# names of different product types\n",
    "print(data['product_type_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2d2f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ef8026b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SHIRT', 167794),\n",
       " ('APPAREL', 3549),\n",
       " ('BOOKS_1973_AND_LATER', 3336),\n",
       " ('DRESS', 1584),\n",
       " ('SPORTING_GOODS', 1281),\n",
       " ('SWEATER', 837),\n",
       " ('OUTERWEAR', 796),\n",
       " ('OUTDOOR_RECREATION_PRODUCT', 729),\n",
       " ('ACCESSORY', 636),\n",
       " ('UNDERWEAR', 425)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the 10 most frequent product_type_names.\n",
    "product_type_count = Counter(list(data['product_type_name']))\n",
    "product_type_count.most_common(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292fd692",
   "metadata": {},
   "source": [
    "Basic stats for the feature: brand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83d7cb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     182987\n",
      "unique     10577\n",
      "top         Zago\n",
      "freq         223\n",
      "Name: brand, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# there are 10577 unique brands\n",
    "print(data['brand'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91534221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Zago', 223),\n",
       " ('XQS', 222),\n",
       " ('Yayun', 215),\n",
       " ('YUNY', 198),\n",
       " ('XiaoTianXin-women clothes', 193),\n",
       " ('Generic', 192),\n",
       " ('Boohoo', 190),\n",
       " ('Alion', 188),\n",
       " ('Abetteric', 187),\n",
       " ('TheMogan', 187)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brand_count = Counter(list(data['brand']))\n",
    "brand_count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ecc140",
   "metadata": {},
   "source": [
    "Basic stats for the feature: color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2705ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     64956\n",
      "unique     7380\n",
      "top       Black\n",
      "freq      13207\n",
      "Name: color, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['color'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bdfd893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, 118182),\n",
       " ('Black', 13207),\n",
       " ('White', 8616),\n",
       " ('Blue', 3570),\n",
       " ('Red', 2289),\n",
       " ('Pink', 1842),\n",
       " ('Grey', 1499),\n",
       " ('*', 1388),\n",
       " ('Green', 1258),\n",
       " ('Multi', 1203)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_count = Counter(list(data['color']))\n",
    "color_count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e49e505",
   "metadata": {},
   "source": [
    "Basic stats for the feature: formatted_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "013f2a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count      28395\n",
      "unique      3135\n",
      "top       $19.99\n",
      "freq         945\n",
      "Name: formatted_price, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['formatted_price'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83f9996e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, 154743),\n",
       " ('$19.99', 945),\n",
       " ('$9.99', 749),\n",
       " ('$9.50', 601),\n",
       " ('$14.99', 472),\n",
       " ('$7.50', 463),\n",
       " ('$24.99', 414),\n",
       " ('$29.99', 370),\n",
       " ('$8.99', 343),\n",
       " ('$9.01', 336)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_count = Counter(list(data['formatted_price']))\n",
    "price_count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be3b71",
   "metadata": {},
   "source": [
    "Basic stats for the feature: title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca69292d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count                                                183138\n",
      "unique                                               175985\n",
      "top       Nakoda Cotton Self Print Straight Kurti For Women\n",
      "freq                                                     77\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['title'].describe())\n",
    "# All of the products have a title.\n",
    "# Titles are fairly descriptive of what the product is.\n",
    "# We use titles extensively in this workshop\n",
    "# as they are short and informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7c404d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('/Users/kaush/Downloads/pickels/180k_apparel_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2249d1",
   "metadata": {},
   "source": [
    "We save data files at every major step in our processing in \"pickle\" files. If we are stuck\n",
    "anywhere (or) if some code takes too long to run on your laptop, we may use the pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ab3adb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points After eliminating price=NULL : 28395\n"
     ]
    }
   ],
   "source": [
    "# consider products which have price information\n",
    "# data['formatted_price'].isnull() => gives the information\n",
    "#about the dataframe row's which have null values price == None|Null\n",
    "data = data.loc[~data['formatted_price'].isnull()]\n",
    "print('Number of data points After eliminating price=NULL :', data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0215bc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points After eliminating color=NULL : 28385\n"
     ]
    }
   ],
   "source": [
    "# consider products which have color information\n",
    "# data['color'].isnull() => gives the information about the dataframe row's which have null values price == None|Null\n",
    "data =data.loc[~data['color'].isnull()]\n",
    "print('Number of data points After eliminating color=NULL :', data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01918b37",
   "metadata": {},
   "source": [
    "We brought down the number of data points from 183K to 28K so that it is easy to run this on colab as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "727c61ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('/Users/kaush/Downloads/pickels/28k_apparel_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e7999a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom PIL import Image\\nimport requests\\nfrom io import BytesIO\\nfor index, row in images.iterrows():\\nurl = row['large_image_url']\\nresponse = requests.get(url)\\nimg = Image.open(BytesIO(response.content))\\nimg.save('images/28k_images/'+row['asin']+'.jpeg')\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can download all these 28k images using this code below.\n",
    "# We do NOT need to run this code and hence it is commented.\n",
    "'''\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "for index, row in images.iterrows():\n",
    "url = row['large_image_url']\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img.save('images/28k_images/'+row['asin']+'.jpeg')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ddca03",
   "metadata": {},
   "source": [
    "# Remove near duplicate items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6a2e8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2325\n"
     ]
    }
   ],
   "source": [
    "# find number of products that have duplicate titles.\n",
    "print(sum(data.duplicated('title')))\n",
    "# we have 2325 products which have same title but different color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f860effa",
   "metadata": {},
   "source": [
    "These shirts are exactly same except in size (S, M,L,XL) :B00AQ4GMCK\n",
    "\n",
    ":B00AQ4GMTS\n",
    "\n",
    ":B00AQ4GMLQ\n",
    "\n",
    ":B00AQ4GN3I\n",
    "\n",
    "These shirts exactly same except in color :B00G278GZ6\n",
    "\n",
    ":B00G278W6O\n",
    "\n",
    ":B00G278Z2A\n",
    "\n",
    ":B00G2786X8\n",
    "\n",
    "In our data there are many duplicate products like the above examples, we need to de-dupe\n",
    "them for better results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876f45a7",
   "metadata": {},
   "source": [
    "# Remove duplicates : Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ffff14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>brand</th>\n",
       "      <th>color</th>\n",
       "      <th>medium_image_url</th>\n",
       "      <th>product_type_name</th>\n",
       "      <th>title</th>\n",
       "      <th>formatted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B004GSI2OS</td>\n",
       "      <td>FeatherLite</td>\n",
       "      <td>Onyx Black/ Stone</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>Featherlite Ladies' Long Sleeve Stain Resistan...</td>\n",
       "      <td>$26.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B012YX2ZPI</td>\n",
       "      <td>HX-Kingdom Fashion T-shirts</td>\n",
       "      <td>White</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>Women's Unique 100% Cotton T - Special Olympic...</td>\n",
       "      <td>$9.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B001LOUGE4</td>\n",
       "      <td>Fitness Etc.</td>\n",
       "      <td>Black</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>Ladies Cotton Tank 2x1 Ribbed Tank Top</td>\n",
       "      <td>$11.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>B003BSRPB0</td>\n",
       "      <td>FeatherLite</td>\n",
       "      <td>White</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>FeatherLite Ladies' Moisture Free Mesh Sport S...</td>\n",
       "      <td>$20.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>B014ICEDNA</td>\n",
       "      <td>FNC7C</td>\n",
       "      <td>Purple</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>Supernatural Chibis Sam Dean And Castiel Short...</td>\n",
       "      <td>$7.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          asin                        brand              color  \\\n",
       "4   B004GSI2OS                  FeatherLite  Onyx Black/ Stone   \n",
       "6   B012YX2ZPI  HX-Kingdom Fashion T-shirts              White   \n",
       "11  B001LOUGE4                 Fitness Etc.              Black   \n",
       "15  B003BSRPB0                  FeatherLite              White   \n",
       "21  B014ICEDNA                        FNC7C             Purple   \n",
       "\n",
       "                                     medium_image_url product_type_name  \\\n",
       "4   https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "6   https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "11  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "15  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "21  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "\n",
       "                                                title formatted_price  \n",
       "4   Featherlite Ladies' Long Sleeve Stain Resistan...          $26.26  \n",
       "6   Women's Unique 100% Cotton T - Special Olympic...           $9.99  \n",
       "11             Ladies Cotton Tank 2x1 Ribbed Tank Top          $11.99  \n",
       "15  FeatherLite Ladies' Moisture Free Mesh Sport S...          $20.54  \n",
       "21  Supernatural Chibis Sam Dean And Castiel Short...           $7.50  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5f01a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted=data[data['title'].apply(lambda x : len(x.split())>4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84164ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removal of products with short description: 27949\n"
     ]
    }
   ],
   "source": [
    "print(\"After removal of products with short description:\" , data_sorted.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9bd6328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaush\\AppData\\Local\\Temp\\ipykernel_17556\\416530081.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_sorted.sort_values('title',inplace=True, ascending=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>brand</th>\n",
       "      <th>color</th>\n",
       "      <th>medium_image_url</th>\n",
       "      <th>product_type_name</th>\n",
       "      <th>title</th>\n",
       "      <th>formatted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61973</th>\n",
       "      <td>B06Y1KZ2WB</td>\n",
       "      <td>Éclair</td>\n",
       "      <td>Black/Pink</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>Éclair Women's Printed Thin Strap Blouse Black...</td>\n",
       "      <td>$24.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133820</th>\n",
       "      <td>B010RV33VE</td>\n",
       "      <td>xiaoming</td>\n",
       "      <td>Pink</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>xiaoming Womens Sleeveless Loose Long T-shirts...</td>\n",
       "      <td>$18.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81461</th>\n",
       "      <td>B01DDSDLNS</td>\n",
       "      <td>xiaoming</td>\n",
       "      <td>White</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>xiaoming Women's White Long Sleeve Single Brea...</td>\n",
       "      <td>$21.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75995</th>\n",
       "      <td>B00X5LYO9Y</td>\n",
       "      <td>xiaoming</td>\n",
       "      <td>Red Anchors</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>xiaoming Stripes Tank Patch/Bear Sleeve Anchor...</td>\n",
       "      <td>$15.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151570</th>\n",
       "      <td>B00WPJG35K</td>\n",
       "      <td>xiaoming</td>\n",
       "      <td>White</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>xiaoming Sleeve Sheer Loose Tassel Kimono Woma...</td>\n",
       "      <td>$14.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              asin     brand        color  \\\n",
       "61973   B06Y1KZ2WB    Éclair   Black/Pink   \n",
       "133820  B010RV33VE  xiaoming         Pink   \n",
       "81461   B01DDSDLNS  xiaoming        White   \n",
       "75995   B00X5LYO9Y  xiaoming  Red Anchors   \n",
       "151570  B00WPJG35K  xiaoming        White   \n",
       "\n",
       "                                         medium_image_url product_type_name  \\\n",
       "61973   https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "133820  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "81461   https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "75995   https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "151570  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "\n",
       "                                                    title formatted_price  \n",
       "61973   Éclair Women's Printed Thin Strap Blouse Black...          $24.99  \n",
       "133820  xiaoming Womens Sleeveless Loose Long T-shirts...          $18.19  \n",
       "81461   xiaoming Women's White Long Sleeve Single Brea...          $21.58  \n",
       "75995   xiaoming Stripes Tank Patch/Bear Sleeve Anchor...          $15.91  \n",
       "151570  xiaoming Sleeve Sheer Loose Tassel Kimono Woma...          $14.32  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Sort the whole data based on title (alphabetical order of title)\n",
    "data_sorted.sort_values('title',inplace=True, ascending=False)\n",
    "data_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ee0da",
   "metadata": {},
   "source": [
    "Some examples of dupliacte titles that differ only in the last few word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "519fc08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i,row in data_sorted.iterrows():\n",
    "    indices.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b241656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5592a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1_dedupe_asins=[]\n",
    "i=0\n",
    "j=0\n",
    "num_data_points=data_sorted.shape[0]\n",
    "while i<num_data_points and j<num_data_points:\n",
    "    previous_i=i\n",
    "    # store the list of words of ith string in a, ex: a = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'X-Large']\n",
    "    a=data['title'].loc[indices[i]].split()\n",
    "    # search for the similar products sequentially\n",
    "    j=i+1\n",
    "    \n",
    "    \n",
    "    while j<num_data_points:\n",
    "        b=data['title'].loc[indices[j]].split()\n",
    "        # store the maximum length of two strings\n",
    "        length = max(len(a), len(b))\n",
    "        # count is used to store the number of words that are matched in both strings\n",
    "        count = 0\n",
    "        # itertools.zip_longest(a,b): will map the corresponding words in both strings, it will appened None in case of unequal strings\n",
    "        # example: a =['a', 'b', 'c', 'd']\n",
    "        # b = ['a', 'b', 'd']\n",
    "        # itertools.zip_longest(a,b): will give [('a','a'), ('b','b'), ('c','d'), ('d', None)]\n",
    "        \n",
    "        \n",
    "        for k in itertools.zip_longest(a,b):\n",
    "            if (k[0]==k[1]):\n",
    "                count += 1\n",
    "                \n",
    "            # if the number of words in which both strings differ are > 2 , we are considering it as those two apperals are different\n",
    "            # if the number of words in which both strings differ are < 2 , we are considering it as those two apperals are same, hence we are ignoring them\n",
    "        if (length - count) > 2:\n",
    "            # number of words in which both sensences differ\n",
    "            # if both strings are differ by more than 2 words we include the 1st string index\n",
    "            stage1_dedupe_asins.append(data_sorted['asin'].loc[indices[i]])\n",
    "            \n",
    "            # if the comaprision between is between num_data_points, num_data_points-1 strings and they differ in more than 2 words we include both\n",
    "            if j == num_data_points-1: \n",
    "                stage1_dedupe_asins.append(data_sorted['asin'].loc[indices[j]])\n",
    "            \n",
    "            # start searching for similar apperals corresponds 2nd string\n",
    "            i = j\n",
    "            break\n",
    "        else:\n",
    "            j += 1\n",
    "    if previous_i == i:\n",
    "        break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63a3de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['asin'].isin(stage1_dedupe_asins)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cdf2cc",
   "metadata": {},
   "source": [
    "We removed the dupliactes which differ only at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "638cfd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points :  17593\n"
     ]
    }
   ],
   "source": [
    "print('Number of data points : ', data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3c7b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('/Users/kaush/Downloads/pickels/17k_apperal_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bcb651",
   "metadata": {},
   "source": [
    "# Remove duplicates : Part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29663a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code snippet takes significant amount of time.\n",
    "# O(n^2) time.\n",
    "# Takes about an hour to run on a decent computer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36398347",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=[]\n",
    "for i,row in data.iterrows():\n",
    "    indices.append(i)\n",
    "    \n",
    "stage2_dedupe_asins=[]\n",
    "while len(indices)!=0:\n",
    "    i=indices.pop()\n",
    "    stage2_dedupe_asins.append(data['asin'].loc[i])\n",
    "    # consider the first apperal's title\n",
    "    a=data['title'].loc[i].split()\n",
    "    # store the list of words of ith string in a, ex: a = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'X-Large']\n",
    "    \n",
    "    for j in indices:\n",
    "        b=data['title'].loc[j].split()\n",
    "        # store the list of words of jth string in b, ex: b = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'X-Large']\n",
    "        length = max(len(a),len(b))\n",
    "        count = 0\n",
    "        for k in itertools.zip_longest(a,b):\n",
    "            if (k[0]==k[1]):\n",
    "                count += 1\n",
    "        \n",
    "        if (length - count) < 3:\n",
    "            indices.remove(j)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f152ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from whole previous products we will consider only\n",
    "# the products that are found in previous cell\n",
    "data = data.loc[data['asin'].isin(stage2_dedupe_asins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac5fcc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points after stage two of dedupe:  16435\n"
     ]
    }
   ],
   "source": [
    "print('Number of data points after stage two of dedupe: ',data.shape[0])\n",
    "# from 17k apperals we reduced to 16k apperals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e35a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('/Users/kaush/Downloads/pickels/16k_apperal_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a786602",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d76381e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_pickle('/Users/kaush/Downloads/pickels/16k_apperal_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "396e4c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltkNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ------------------ --------------------- 0.7/1.5 MB 14.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 18.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 16.0 MB/s eta 0:00:00\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 0.0/96.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 96.6/96.6 kB ? eta 0:00:00\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     ---------------------------------------- 0.0/298.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 298.0/298.0 kB 9.3 MB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp311-cp311-win_amd64.whl (267 kB)\n",
      "     ---------------------------------------- 0.0/267.7 kB ? eta -:--:--\n",
      "     ------------------------------------- 267.7/267.7 kB 17.2 MB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 78.5/78.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\kaush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.3 joblib-1.2.0 nltk-3.8.1 regex-2022.10.31 tqdm-4.64.1\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d32ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f9ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f9b4511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kaush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1bd8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a73a9a52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def nlp_preprocessing(total_text,index,column):\n",
    "    if type(total_text) is not int:\n",
    "        string=\"\"\n",
    "        for words in total_text.split():\n",
    "            word = (\"\".join(e for e in words if e.isalnum())) \n",
    "            #remoe the special character\n",
    "            word=word.lower()\n",
    "            if not word in stop_words:\n",
    "                word=word+\" \"\n",
    "                string= string+word\n",
    "        data[column][index]=string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa5d5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take each title and we text-preprocess it.\n",
    "for index, row in data.iterrows():\n",
    "    nlp_preprocessing(row['title'], index, 'title')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c66732",
   "metadata": {},
   "source": [
    "# Text Based Product Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f59ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_pickle('/Users/kaush/Downloads/pickels/16k_apperal_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27b7a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display an image\n",
    "def display_img(url,ax,fig):\n",
    "    # we get the url of the apparel and download it\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    # we will display it in notebook\n",
    "    plt.imshow(img)\n",
    "    \n",
    "#plotting code to understand the algorithm's decision.\n",
    "def plot_heatmap(keys, values, labels, url, text):\n",
    "    # keys: list of words of recommended title\n",
    "    # values: len(values) == len(keys), values(i) represents the occurence of the word keys(i)\n",
    "    # labels: len(labels) == len(keys), the values of labels depends on the model we are using\n",
    "    # if model == 'bag of words': labels(i) = values(i)\n",
    "    # if model == 'tfidf weighted bag of words':labels(i) = tfidf(keys(i))\n",
    "    # if model == 'idf weighted bag of words':labels(i) = idf(keys(i))\n",
    "    # url : apparel's url\n",
    "    # we will devide the whole figure into two parts\n",
    "    gs = gridspec.GridSpec(2, 2, width_ratios=[4,1], height_ratios=[4,1])\n",
    "    fig = plt.figure(figsize=(25,3))\n",
    "    # 1st, ploting heat map that represents the count of commonly ocurred words in title2\n",
    "    ax = plt.subplot(gs[0])\n",
    "    # it displays a cell in white color if the word is intersection(lis of words of title1 and list of words of title2), in black if not\n",
    "    ax = sns.heatmap(np.array([values]), annot=np.array([labels]))\n",
    "    ax.set_xticklabels(keys) # set that axis labels as the words of title\n",
    "    ax.set_title(text) # apparel title\n",
    "    # 2nd, plotting image of the the apparel\n",
    "    ax = plt.subplot(gs[1])\n",
    "    # we don't want any grid lines for image and no labels on x-axis and y-axis\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    # we call dispaly_img based with paramete url\n",
    "    display_img(url, ax, fig)\n",
    "    # displays combine figure ( heat map and image together)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_heatmap_image(doc_id, vec1, vec2, url, text, model):\n",
    "    # doc_id : index of the title1\n",
    "    # vec1 : input apparels's vector, it is of a dict type {word:count}\n",
    "    # vec2 : recommended apparels's vector, it is of a dict type {word:count}\n",
    "    \n",
    "    # url : apparels image url\n",
    "    # text: title of recomonded apparel (used to keep title of image)\n",
    "    # model, it can be any of the models,\n",
    "    # 1. bag_of_words\n",
    "    # 2. tfidf\n",
    "    # 3. idf\n",
    "    # we find the common words in both titles, because these only words contribute to the distance between two title vec's\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    # we set the values of non intersecting words to zero, this is just to show the difference in heatmap\n",
    "    for i in vec2:\n",
    "        if i not in intersection:\n",
    "            vec2[i]=0\n",
    "            \n",
    "    # for labeling heatmap, keys contains list of all words in title2\n",
    "    keys = list(vec2.keys())\n",
    "    # if ith word in intersection(lis of words of title1 and list of words of title2): values(i)=count of that word in title2 else values(i)=0\n",
    "    values = [vec2[x] for x in vec2.keys()]\n",
    "    # labels: len(labels) == len(keys), the values of labels depends on the model we are using\n",
    "    # if model == 'bag of words': labels(i) = values(i)\n",
    "    # if model == 'tfidf weighted bag of words':labels(i) = tfidf(keys(i))\n",
    "    # if model == 'idf weighted bag of words':labels(i) = idf(keys(i))\n",
    "    if model == 'bag_of_words':\n",
    "        labels = values\n",
    "    elif model == 'tfidf':\n",
    "        labels = []\n",
    "        for x in vec2.keys():\n",
    "        # tfidf_title_vectorizer.vocabulary_ it contains all the words in the corpus\n",
    "        # tfidf_title_features[doc_id, index_of_word_in_corpus] will give the tfidf value of word in given document (doc_id)\n",
    "            if x in tfidf_title_vectorizer.vocabulary_:\n",
    "                labels.append(tfidf_title_features[doc_id, tfidf_title_vectorizer.vocabulary_[x]])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    elif model == 'idf':\n",
    "        labels = []\n",
    "        for x in vec2.keys():\n",
    "            # idf_title_vectorizer.vocabulary_ it contains all the words in the corpus\n",
    "            # idf_title_features[doc_id, index_of_word_in_corpus] will give the idf value of word in given document (doc_id)\n",
    "            if x in idf_title_vectorizer.vocabulary_:\n",
    "                labels.append(idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[x]])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    plot_heatmap(keys, values, labels, url, text)\n",
    "\n",
    "\n",
    "    # this function gets a list of wrods along with the frequency of each\n",
    "# word given \"text\"\n",
    "def text_to_vector(text):\n",
    "    word = re.compile(r'\\w+')\n",
    "    words = word.findall(text)\n",
    "    # words stores list of all words in given string, you can try 'words = text.split()' this will also gives same result\n",
    "    return Counter(words) # Counter counts the occurence of each word in list, it returns dict type object {word1:count}\n",
    "\n",
    "\n",
    "def get_result(doc_id, content_a, content_b, url, model):\n",
    "    text1 = content_a\n",
    "    text2 = content_b\n",
    "    # vector1 = dict{word11:#count, word12:#count, etc.}\n",
    "    vector1 = text_to_vector(text1)\n",
    "    # vector1 = dict{word21:#count, word22:#count, etc.}\n",
    "    vector2 = text_to_vector(text2)\n",
    "    plot_heatmap_image(doc_id, vector1, vector2, url, text2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac1e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac95e28e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
